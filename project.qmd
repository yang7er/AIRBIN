---
format: ipynb
execute: 
  eval: false
---

```{r}
library("tidyverse")
library("lubridate")
library("magrittr")
library("zoo")
library("data.table")
library("keras")
library("tensorflow")
library("RCurl")
library("caret")
library("reticulate")
library("httr")
library("jsonlite")
reticulate::use_condaenv("stat_ml")
reticulate::py_run_string("import tensorflow as tf")
reticulate::py_run_string("tf.random.set_seed(1)")
```

## Data Pre-Preprocessing

This part merges the AirBnB source data and downloads the public holiday data from the Victorian government API. As this part is based on external downloading providers with potentially variable links, we provide the final data as a zip file in the repository. The code is provided for completeness.

```{r}
#| eval: false
## DO NOT RUN!

# read in and combine "Inside AirBnB" data
out_list <- rep_len(NA, 4) %>% as.list() %>% "names<-"(c("0922", "1222", "0323","0623"))
for(suffix in c("0922", "1222", "0323","0623")){
  listings <- read_csv(gzfile(paste0("listings_", suffix,".csv.gz")),
                       show_col_types = FALSE)
  calendar <- read_csv(gzfile(paste0("calendar_", suffix,".csv.gz")),
                       show_col_types = FALSE)

  out_list[[suffix]] <- listings %>%
      select(id, beds) %>%
      left_join(calendar %>% select(listing_id, date, price), by = c("id" = "listing_id"))
}
full_data <- rbindlist(out_list, idcol = "dataset")

# reformat price and date
full_data %<>%
  mutate(price = price %>%
            str_replace_all("\\$", "") %>%
            str_replace_all(",", "") %>%
            as.double(),
         date = as_date(date)) %>% 
  filter(date != max(full_data$date, na.rm = T)) 

# focus on 4 and 6 bed listings
full_data %>% write_csv(gzfile("full_data.csv.gz","wb"))
```

```{r}
#| eval: false
## DO NOT RUN!

# Define the base URL
base_url <- "https://wovg-community.gateway.prod.api.vic.gov.au/vicgov/v2.0/dates"

# Initialize an empty list to store the results
all_holidays <- list()

# Define the start and end dates
start_date <- as.Date("2022-01-01")
end_date <- as.Date("2025-12-31")

# Construct the URL for the current page
url <- modify_url(
  base_url,
  query = list(
    type = "PUBLIC_HOLIDAY,SCHOOL_TERM",
    from_date = start_date,
    to_date = end_date
  )
)

# Make an HTTP GET request to the API
source("components/apikey.R")
response <- GET(url, add_headers(apikey = apikey))

# Check if the response was successful
if (http_status(response)$message == "Success: (200) OK") {
  
  # Parse the JSON response
  holiday_data <- content(response)$dates

  readr::write_csv(data.table::rbindlist(holiday_data), "public_holidays.csv")
  
} else {
  # If the response is not successful, stop the loop
  stop("Error: Unable to retrieve data from the API.")
}
```

## Data Preprocessing

```{r}
# download and extract data
download.file("https://api.onedrive.com/v1.0/shares/s!AjXuK6Vn5knCitJy_qjyBha8mNjyzQ/root/content", "data.zip")
unzip("data.zip")
full_data <- read_csv("full_data.csv")
```

```{r}
# read in information on public holidays
public_holidays <- read_csv("public_holidays.csv", show_col_types = FALSE) %>%
  filter(type == "PUBLIC_HOLIDAY") %>%
  summarise(date = ymd(date)) %>%
  arrange(date) %>%
  pull()
```

```{r}
# read in information on school holidays
school_holidays <- read_csv("public_holidays.csv", show_col_types = FALSE) %>%
  filter(type == "SCHOOL_TERM") %>%
  mutate(term = str_extract(name, "(?<=Term )[1-4]"),
         year = str_extract(name, "\\d{4}"),
         tmp = ifelse(str_detect(name, "Start"), "semester_start", "semester_end")) %>%
  pivot_wider(names_from = tmp, values_from = date) %>%
  group_by(term, year) %>%
    summarise(across(c(semester_start, semester_end), ~ first(na.omit(.)))) %>%
  ungroup() %>%
  arrange(year, term) %>%
  mutate(holiday_start = semester_end + 1,
         holiday_end = lead(semester_start) - 1) %>% 
  summarise(holidays = interval(holiday_start, holiday_end)) %>%
  pull()
```

```{r}
# create a function that performs feature creation, returning a data frame
feature_creation <- function(data){

  # create a mean price variable
  data %<>%
    group_by(date) %>%
    summarize(mean_price = mean(price, na.rm = TRUE))

  # scaling output   
  min_max_scale <- function(data, feature_range=c(0, 1)) {
    min_val <- min(data, na.rm = TRUE)
    max_val <- max(data, na.rm = TRUE)
    
    scale_min <- feature_range[1]
    scale_max <- feature_range[2]
    
    scaled_data <- (data - min_val) / (max_val - min_val) * (scale_max - scale_min) + scale_min
    list(scaled_data=scaled_data, min_val=min_val, max_val=max_val)
    } 

  # scale if not all zero (i.e. prediction of future prices)
  if(!all(data$mean_price == 0)) data$mean_price <- min_max_scale(data$mean_price)[[1]]

  # adding lagged outcomes in column
  eval(parse(text = paste0("data %<>% mutate(", paste("lag(mean_price,", 1:28, ")", collapse = ", ", sep = ""), ")")))

  # create several external variables of seasonality
  data %<>% 
    mutate(
      time = as.double(date),
      year = year(date),
      month = month(date),
      day = day(date),
      day_of_year = yday(date),
      weekday = wday(date),
      month_scaled = (month - 1) / 11,
      day_scaled = (day - 1) / 30,
      weekday_scaled = (weekday - 1) / 6,
      day_of_year_scaled = (day_of_year - 1) / 364,
      year_scaled = (year - 2022) / (2025 - 2022),
      time_scaled = (time - as.double(ymd("20241231"))) /
                    (as.double(ymd("20241231")) - as.double(ymd("20220101"))))

  within_any <- function(date, intervals){
      map(intervals, ~ date %within% .x)
  }
  
  # adding holiday information to the data
  data %<>%
    mutate(public_holiday = date %in% public_holidays,
           public_holidays = lag(public_holiday, 1) | public_holiday | lead(public_holiday, 1),
           school_holiday = map_dfc(school_holidays, ~ date %within% .x) %>% mutate(a = pmap(., any)) %>% pull("a") %>% as.logical(),
           school_holiday = lag(school_holiday, 1) | school_holiday | lead(school_holiday, 1),
          across(c(month, day, weekday), factor))

  # first-order difference of lagged outcome at week level
  #data %<>% 
  #  mutate(across(mean_price, ~ .x - lag(.x, 7)))

    # One-hot encoding for categorical variables like 'day_of_week' and 'month'
  create_dummy_variables <- function(in_data, factor_col, levels) {
      # Create an empty data frame to store dummy variables
      dummy_df <- data.frame(matrix(0, nrow = nrow(in_data), ncol = length(levels) - 1))

      # Set column names for the dummy variables
      colnames(dummy_df) <- paste0(factor_col, tail(levels, -1))

      # Fill in the dummy variables
      for (i in 1:(length(levels) - 1)) {
        dummy_df[, i] <- as.integer(in_data[[factor_col]] == levels[i + 1])
      }

      # Combine the original data with the dummy variables
      in_data <- cbind(in_data, dummy_df)

      # Remove the original factor variable
      #in_data[[factor_col]] <- NULL

      return(in_data)
    }

  data %<>% create_dummy_variables(., "day", 1:31)
  data %<>% create_dummy_variables(., "weekday", 1:7)
  data %<>% create_dummy_variables(., "month", 1:12)
  #df %<>% create_dummy_variables(., "year", 2022:2024)
  #df %<>% select(-date)

  tibble(data)
}
```

```{r}
descale_prices <- function(prices, full_data){
  # a function to reverse scaling
  rev_min_max <- function(data, min_val, max_val){
    data * (max_val - min_val) + min_val
  }

  # get the anchor values for reverse scaling
  tmp <- full_data %>%
    group_by(date) %>%
    summarise(mean_price = mean(price), date = unique(date))
  tmp_min <- tmp$mean_price %>% min(na.rm = T); tmp_max <- tmp$mean_price %>% max(na.rm = T)

  rev_min_max(prices, tmp_min, tmp_max)
}
```

```{r}
# implement a time series split
train_test_val_split <- function(data, k){
  k <- k + 2
  rows <- nrow(data)
  split_size <- rows %/% k

  indices <- matrix(NA, nrow = rows, ncol = 3)
  for(i in 1:(k-2)){
    indices[i,1] <- i * split_size
    indices[i,2] <- (i + 1) * split_size
    indices[i,3] <- (i + 2) * split_size
    if(i+2==k){
      indices[i,3] <- rows
    }
  }

  map(1:(k-2), ~ list(data[1:indices[.x,1],], 
                      data[(indices[.x,1] + 1):indices[.x,2],], 
                      data[(indices[.x,2] + 1):indices[.x,3],]))
}
```

```{r}
k_folds <- 3

# filter data, create lagged and calendar features, drop initial NAs
analysis_data <- full_data %>% 
  filter(date < as_date("2023-10-01")) %>% 
  filter(beds %in% c(4, 6)) %>%
  feature_creation() %>%
  drop_na()

# get the lagged time series input, split and add dimension
X_lagged <- analysis_data %>% 
  select(`lag(mean_price, 1)`:`lag(mean_price, 28)`) %>% 
  as.matrix() %>%
  train_test_val_split(k_folds) %>%
  map(., ~ map(.x, ~ .x %>% `dim<-`(c(dim(.)[1], dim(.)[-1], 1))))

# get the external input, split and add dimension
X_external <- analysis_data %>% 
  select(public_holiday, school_holiday, year_scaled, day2:month12) %>% 
  as.matrix() %>%
  train_test_val_split(k_folds)

# get the output data
y <- analysis_data %>% 
  select(mean_price) %>%
  train_test_val_split(k_folds) %>% 
  map(., ~ map(.x, pull))
```

## Model utilities

```{r}
# a function implementing hyperparameter grid search
grid_search <- function(X, y, model, hyperparameter_grid, k){
  if(file.exists("grid_search/hyperparameter_grid.csv")){
    hyperparameter_grid <- read_csv("grid_search/hyperparameter_grid.csv")
    start <- nrow(hyperparameter_grid %>% drop_na()) + 1
  } else {
    hyperparameter_grid <- expand.grid(hyperparameter_grid)
    hyperparameter_grid %<>% add_column(score_1 = NA, score_2 = NA, score_3 = NA)
    start <- 1
  }

  print("--- commencing hyperparameter search ---")
  # prepare output grid
  #results <- as.list(rep_len(NA, 10))
  # loop over hyperparameter grid
  for(i in start:nrow(hyperparameter_grid)){
    print(paste0("*** hyperparameter search: ", i, " of ", nrow(hyperparameter_grid), " ***"))
    # prepare list for results of k folds
    #results[[((i-1)%%10+1)]] <- as.list(rep_len(NA, k))
    # loop over folds
    for(j in 1:k){
      print(paste0("~~~ fold: ", j, " of ", k, " ~~~"))
      mod <- model(hyperparameter_grid[i,] %>% as.list()) 
      mod %>% fit(
        x = list(timeseries_input = X[[1]][[j]][[1]],
                 external_input = X[[2]][[j]][[1]]),
        y = y[[j]][[1]],
        epochs = 100,
        batch_size = 32,
        verbose = 0,
        callbacks = list(
          callback_model_checkpoint(filepath = "checkpoints_grid_search.h5",
                                    save_best_only = T, save_weights_only= T),
          callback_reduce_lr_on_plateau(monitor = "val_loss", patience = 10, factor = 0.1),
          callback_early_stopping(monitor = "val_loss", patience = 15)),
        validation_data = list(list(timeseries_input = X[[1]][[j]][[2]],
                                    external_input = X[[2]][[j]][[2]]), 
                               y[[j]][[2]])
      )
      #results[[((i-1)%%10+1)]][[j]] <- 
      load_model_weights_tf(mod, "checkpoints_grid_search.h5")
      hyperparameter_grid[i,paste0("score_",j)] <- 
        evaluate(mod, list(timeseries_input = X[[1]][[j]][[3]],
                           external_input = X[[2]][[j]][[3]]), 
                     y[[j]][[3]], verbose = 0)
      
    }
    # store results every 10 iterations
    if(((i-1)%%10)==0){
      hyperparameter_grid %>% write_csv("grid_search/hyperparameter_grid.csv")
      #saveRDS(results, file = paste0("grid_search/grid_search_lstmnet_", (i%/%10),".rds"))
      #results <- as.list(rep_len(NA, 10))
    }
  }
}
```

```{r}
final_fit <- function(X, y, hyperparameter_list, model, model_name){

  mod <- model(hyperparameter_list)

  history <- mod %>%
    fit(
      x = list(timeseries_input = tail(X[[1]], 1)[[1]][[1]], 
               external_input = tail(X[[2]], 1)[[1]][[1]]),
      y = tail(y, 1)[[1]][[1]],
      epochs = 100,
      batch_size = 32,
      verbose = 0,
      callbacks = list(
        callback_model_checkpoint(filepath = paste0("checkpoints_", model_name, ".h5"),
                                  save_best_only = T, save_weights_only= F),
        callback_reduce_lr_on_plateau(monitor = "val_loss",
        patience = 5, factor = 0.1),
        callback_early_stopping(monitor = "val_loss", patience = 15)),
      validation_data = list(list(timeseries_input = tail(X[[1]], 1)[[1]][[2]], 
                                  external_input = tail(X[[2]], 1)[[1]][[2]]), 
                             tail(y, 1)[[1]][[2]])
    )
    
    load_model_weights_tf(mod, paste0("checkpoints_", model_name, ".h5"))
    return(list(mod, history))
}
```

```{r}
final_predict <- function(mod, analysis_data, start_date = "2023-10-01", end_date = "2024-06-30"){

  # a function to generate a sequence of dates
  generate_date_sequence <- function(start_date, end_date, interval) {
    # Convert input strings to date objects
    start_date <- as.Date(start_date)
    end_date <- as.Date(end_date)

    # Create a sequence of dates
    date_sequence <- seq(start_date, end_date, by = interval)

    return(date_sequence)
  }

  # create a data frame with the dates for the prediction period
  raster_predict <- expand_grid(date = generate_date_sequence(start_date, end_date, "1 day"), 
                                price = 0) %>% 
    feature_creation()


  # get the lagged time series input, split and add dimension
  X_lagged_predict <- raster_predict %>% 
    select(`lag(mean_price, 1)`:`lag(mean_price, 28)`) %>% 
    as.matrix() %>% 
    `dim<-`(c(dim(.)[1], dim(.)[-1], 1))
  X_lagged_predict[1,,] <- analysis_data %>% 
    filter(date == (ymd(start_date) - 1)) %>%
    select(`lag(mean_price, 1)`:`lag(mean_price, 28)`) %>%
    as.matrix()

  # get the external input, split and add dimension
  X_external_predict <- raster_predict %>% 
    select(public_holiday, school_holiday, year_scaled, day2:month12) %>% 
    as.matrix()

  X_predict <- list(X_lagged_predict, X_external_predict)

  # get the output data
  y_predict <- raster_predict %>% 
    select(date, mean_price)

  for(i in 1:(nrow(X_predict[[1]]) - 1)){
    #
    y_predict$mean_price[[i]] <- mod %>% predict(list(X_predict[[1]][i,,,drop=F], X_predict[[2]][i,,drop=F]), verbose = 0)
    #
    X_predict[[1]][i+1,,] <- c(y_predict$mean_price[[i]], X_predict[[1]][i,1:27,])
  }

  y_predict
}
```

## A LSTM neural network using external features and lagged time series data

```{r}
source("components/lstmnet.R")
lstmnet <- model
```

### Hyperparameter grid search

```{r}
hyperparameters_grid <- list(

    # LSTM hyperparameters
    lstm_filters = c(64, 256),             # Number of units/filters in the LSTM layer

    # Dense layers for external features
    n_hidden_layers = c(1, 3),                   # Number of hidden layers for external features
    init_filters = c(128, 512),                # Number of neurons/filters in the first dense layer
    filter_reduction_factor = c(2, 3),              # Reduction factor for neurons/filters in subsequent layers
    init_dropout = c(0.1, 0.2),                # Initial dropout rate
    dropout_reduction_factor = c(1, 2),        # Reduction factor for dropout in subsequent layers

    # Regularization
    regularize_lambda = c(0.001, 0.1),        # L2 regularization factor for LSTM

    # Model input parameters (fixed)
    n_lags = c(28),                                 # Number of lagged time series data
    n_external_features = c(50),                    # Number of external features

    # Model compilation hyperparameters
    learning_rate = c(0.001, 0.01, 0.1)             # Learning rate for optimizer
)
```

```{r}
#| eval: false
# execute grid search, store results
grid_search(list(X_lagged, X_external), y, lstmnet, hyperparameters_grid, k_folds)
```

```{r}
#| eval: false
# get best hyperparameters from grid search result
best_hyperparameters <- read_csv("grid_search/hyperparameter_grid_lstmnet.csv") %>%
  drop_na() %>% 
  mutate(result = score_1 + score_2 + score_3 * 1/3) %>%
  #pull(result) %>% which.min()
  arrange(result) %>%
  head(1) %>%
  as.list()
```

### Final model

```{r}
# hardcoded so grid search does not have to be rerun
best_hyperparameters <- list(

    # LSTM hyperparameters
    lstm_filters = c(64),             # Number of units/filters in the LSTM layer

    # Dense layers for external features
    n_hidden_layers = c(3),                   # Number of hidden layers for external features
    init_filters = c(128),                # Number of neurons/filters in the first dense layer
    filter_reduction_factor = c(2),              # Reduction factor for neurons/filters in subsequent layers
    init_dropout = c(0.2),                # Initial dropout rate
    dropout_reduction_factor = c(1),        # Reduction factor for dropout in subsequent layers

    # Regularization
    regularize_lambda = c(0.1),        # L2 regularization factor for LSTM

    # Model input parameters (fixed)
    n_lags = c(28),                                 # Number of lagged time series data
    n_external_features = c(50),                    # Number of external features

    # Model compilation hyperparameters
    learning_rate = c(0.001)             # Learning rate for optimizer
)
```

```{r}
run <- final_fit(list(X_lagged, X_external), y, best_hyperparameters, lstmnet, "lstmnet")

plot(run[[2]])
```

```{r}
raster_predict <- final_predict(run[[1]], analysis_data)

final_data <- full_data %>% feature_creation() %>% select(date, mean_price) %>% add_column(type = "actual")
final_data %<>%  
  bind_rows(., raster_predict %>% add_column(type = "lstmnet"))
```

## A 1D-Convolutional Neural Network

```{r}
source("components/convnet.R")
convnet <- model
```

### Final model

```{r}
hyperparameters_grid <- list(

    # 1D Convolution hyperparameters
    conv_filters = c(64),                 # Number of filters in the convolution
    conv_kernel_size = c(3),                  # Size of the convolution kernel
    pool_size = c(3),                            # Max pooling size
    conv_dense_units = c(32),                 # Dense layer size after 1D-Convolution 

    # Dense layers for external features
    n_hidden_layers = c(3),                      # Number of hidden layers for external features
    init_filters = c(128),                   # Number of neurons/filters in the first dense layer
    filter_reduction_factor = c(2),                 # Reduction factor for neurons/filters in subsequent layers
    init_dropout = c(0.2),                   # Initial dropout rate
    dropout_reduction_factor = c(1),           # Reduction factor for dropout in subsequent layers

    # Regularization
    regularize_lambda = c(0.1),           # L2 regularization factor

    # Model input parameters (fixed)
    n_lags = c(28),                                 # Number of lagged time series data
    n_external_features = c(50),                    # Number of external features

    # Model compilation hyperparameters
    learning_rate = c(0.001)               # Learning rate for optimizer

)
```

```{r}
run <- final_fit(list(X_lagged, X_external), y, hyperparameters_grid, convnet, "convnet")

plot(run[[2]])
```

```{r}
raster_predict <- final_predict(run[[1]], analysis_data)

final_data %<>%  
  bind_rows(., raster_predict %>% add_column(type = "convnet"))
```

## Descriptives

```{r}
get_rmse <- function(data, model){
  actual <- data %>% filter(date >= "2023-10-01",
                            date <= "2024-06-09",
                            type == "actual") %>% 
              pull(mean_price)
  predicted <- data %>% filter(date >= "2023-10-01",
                               date <= "2024-06-09",
                               type == model)  %>% 
              pull(mean_price)
  sqrt(mean((actual - predicted)^2))
}

list("lstmnet" = get_rmse(final_data, "lstmnet"),
     "convnet" = get_rmse(final_data, "convnet"))


final_data %>%
    filter(date <= max(date[type=="actual"])) %>%
    ggplot(aes(x = date, y = descale_prices(mean_price, full_data), color = type)) +
      geom_line() +
      theme_classic() +
      theme(text=element_text(family="TT Arial"))
```